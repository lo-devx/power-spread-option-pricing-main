{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection – ENTSO-E API\n",
    "This notebook uses the ENTSO-E Transparency Platform API to pull hourly day-ahead power prices for the UK and France.\n",
    "We will:\n",
    "- Use the official API to fetch historical data\n",
    "- Parse the XML response into a clean DataFrame\n",
    "- Save the cleaned dataset to data/processed/cleaned_UK_FR_prices.csv\n",
    "\n",
    "\n",
    "\n",
    "Note:\n",
    "- ENTSO-E returns data in XML format, not JSON or CSV\n",
    "- Use requests + xml.etree.ElementTree to parse it\n",
    "- Need API key set as a variable\n",
    "- Each API call can return a max of 1 month of data - so we loop monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Lyndon.Odia/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Lyndon.Odia/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "import urllib3\n",
    "import ssl\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(\"/Users/Lyndon.Odia/Desktop/lo-devx/power-spread-option-pricing-main\"))\n",
    "from config import raw_data_dir, processed_data_dir, API_KEY, FR_DOMAIN, START_DATE, END_DATE, FX_GBP_EUR\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Lyndon.Odia/Desktop/lo-devx/power-spread-option-pricing-main/data/raw\n",
      "/Users/Lyndon.Odia/Desktop/lo-devx/power-spread-option-pricing-main/data/processed\n",
      "3faef1ee-b130-4678-9759-4ac9c0af0941 10YFR-RTE------C\n",
      "2025-01-01 00:00:00\n",
      "2025-08-01 23:00:00\n",
      "1.17\n"
     ]
    }
   ],
   "source": [
    "#Check\n",
    "print(raw_data_dir) \n",
    "print(processed_data_dir) \n",
    "print(API_KEY, FR_DOMAIN) \n",
    "print(START_DATE)\n",
    "print(END_DATE)\n",
    "print(FX_GBP_EUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel file name - Uk day ahead prices\n",
    "uk_xlsx = os.path.join(raw_data_dir, \"uk_day_ahead_prices.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(dt: datetime) -> str:\n",
    "\n",
    "    return dt.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "def next_month(dt: datetime) -> datetime:\n",
    "\n",
    "    y, m = dt.year, dt.month\n",
    "\n",
    "    return datetime(y + (m == 12), 1 if m == 12 else m + 1, 1)\n",
    "\n",
    "def month_windows(start: datetime, end: datetime):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Yield (m_start, m_end_exclusive) covering [start, end], with m_end_exclusive set to\n",
    "\n",
    "    00:00 of the first day of the next month, per ENTSO-E best practice.\n",
    "\n",
    "    We’ll clip the last window to end+1hour to be safe, then filter later.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "\n",
    "    while cur <= end:\n",
    "\n",
    "        nxt = next_month(cur)\n",
    "\n",
    "        yield max(cur, start), min(nxt, next_month(end.replace(day=1)))\n",
    "\n",
    "        cur = nxt\n",
    "\n",
    "def parse_resolution_to_timedelta(res_text: str) -> timedelta:\n",
    "\n",
    "    # Expected \"PT60M\" for hourly. You can extend as needed.\n",
    "\n",
    "    # Minimal parser: supports PT{n}M and PT{n}H\n",
    "\n",
    "    if not res_text or not res_text.startswith(\"PT\"):\n",
    "\n",
    "        return timedelta(hours=1)\n",
    "\n",
    "    body = res_text[2:]\n",
    "\n",
    "    if body.endswith(\"M\"):\n",
    "\n",
    "        minutes = int(body[:-1])\n",
    "\n",
    "        return timedelta(minutes=minutes)\n",
    "\n",
    "    if body.endswith(\"H\"):\n",
    "\n",
    "        hours = int(body[:-1])\n",
    "\n",
    "        return timedelta(hours=hours)\n",
    "\n",
    "    # default to 1 hour if unknown\n",
    "\n",
    "    return timedelta(hours=1)\n",
    "\n",
    "def fetch_entsoe_fr_period(start_exclusive: datetime, end_exclusive: datetime) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Request FR day-ahead prices for [start_exclusive, end_exclusive) per ENTSO-E convention.\n",
    "\n",
    "    Parse using each Period's timeInterval start rather than assuming month start.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "\n",
    "        \"securityToken\": API_KEY,\n",
    "\n",
    "        \"documentType\": \"A44\",                 # Day-ahead prices\n",
    "\n",
    "        \"in_Domain\": FR_DOMAIN,\n",
    "\n",
    "        \"out_Domain\": FR_DOMAIN,\n",
    "\n",
    "        \"periodStart\": fmt(start_exclusive),\n",
    "\n",
    "        \"periodEnd\": fmt(end_exclusive),     \n",
    "\n",
    "    }\n",
    "\n",
    "    r = requests.get(\"https://web-api.tp.entsoe.eu/api\", params=params, timeout=60, verify=False)\n",
    "\n",
    "    r.raise_for_status()\n",
    "\n",
    "    root = ET.fromstring(r.content)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # For each TimeSeries → for each Period → get timeInterval start and resolution\n",
    "    for ts in root.findall(\".//{*}TimeSeries\"):\n",
    "\n",
    "        for period in ts.findall(\".//{*}Period\"):\n",
    "\n",
    "            ti = period.find(\"{*}timeInterval\")\n",
    "\n",
    "            p_start_text = ti.find(\"{*}start\").text\n",
    "\n",
    "\n",
    "            res_text = period.find(\"{*}resolution\").text if period.find(\"{*}resolution\") is not None else \"PT60M\"\n",
    "\n",
    "            p_start = datetime.fromisoformat(p_start_text.replace(\"Z\",\"\"))\n",
    "\n",
    "            step = parse_resolution_to_timedelta(res_text)\n",
    "\n",
    "            for pt in period.findall(\"{*}Point\"):\n",
    "\n",
    "                pos = int(pt.find(\"{*}position\").text)\n",
    "\n",
    "                price = float(pt.find(\"{*}price.amount\").text)\n",
    "\n",
    "                ts_dt = p_start + (pos - 1) * step\n",
    "\n",
    "                rows.append({\"datetime\": ts_dt, \"FR_price\": price})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Normalize: floor to hour, sort, dedupe\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"]).dt.floor(\"h\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"datetime\"]).sort_values(\"datetime\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def fetch_entsoe_fr_range(start: datetime, end: datetime) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Pulls all hours in [start, end] inclusive, by calling month windows with exclusive month ends.\n",
    "\n",
    "    Then filters to the exact requested window.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # Use month ends at 00:00 of next month to avoid losing last day\n",
    "\n",
    "    for m_start, m_end_excl in month_windows(start, end):\n",
    "\n",
    "        # ENTSO-E wants exclusive end; good practice is 00:00 next day/month\n",
    "\n",
    "        dfm = fetch_entsoe_fr_period(m_start, m_end_excl)\n",
    "\n",
    "        parts.append(dfm)\n",
    "\n",
    "    if not parts:\n",
    "\n",
    "        return pd.DataFrame(columns=[\"datetime\",\"FR_price\"])\n",
    "\n",
    "    df = (pd.concat(parts, ignore_index=True)\n",
    "\n",
    "            .drop_duplicates(subset=[\"datetime\"])\n",
    "\n",
    "            .sort_values(\"datetime\"))\n",
    "\n",
    "    # Filter to the exact range requested, inclusive\n",
    "    df = df[(df[\"datetime\"] >= start) & (df[\"datetime\"] <= end)].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  FR_price\n",
      "0 2025-01-01 00:00:00     18.92\n",
      "1 2025-01-01 01:00:00     16.66\n",
      "2 2025-01-01 02:00:00     13.10\n",
      "3 2025-01-01 03:00:00      5.90\n",
      "4 2025-01-01 04:00:00      9.27\n",
      "                datetime  FR_price\n",
      "4988 2025-08-01 19:00:00    103.02\n",
      "4989 2025-08-01 20:00:00    102.52\n",
      "4990 2025-08-01 21:00:00     95.41\n",
      "4991 2025-08-01 22:00:00     97.80\n",
      "4992 2025-08-01 23:00:00     77.08\n",
      "datetime    datetime64[ns]\n",
      "FR_price           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_fr = fetch_entsoe_fr_range(START_DATE, END_DATE)\n",
    "\n",
    "print(df_fr.head())\n",
    "print(df_fr.tail())\n",
    "print(df_fr.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load UK hourly from Excel -\n",
    "df_uk = pd.read_excel(os.path.join(raw_data_dir, \"UK_day_ahead_prices.xlsx\"))\n",
    "df_uk = df_uk.rename(columns={df_uk.columns[1]: \"UK_price\"})\n",
    "df_uk[\"datetime\"] = pd.to_datetime(df_uk[\"datetime\"]).dt.floor(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR hours: 4993\n",
      "UK hours: 5088\n"
     ]
    }
   ],
   "source": [
    "# check for unique datetime records\n",
    "print(\"FR hours:\", df_fr[\"datetime\"].nunique())\n",
    "print(\"UK hours:\", df_uk[\"datetime\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on datetime (inner join = only matching hours kept)\n",
    "merged_df = pd.merge(\n",
    "  df_fr[[\"datetime\", \"FR_price\"]],\n",
    "  df_uk[[\"datetime\", \"UK_price\"]],\n",
    "  on=\"datetime\",\n",
    "  how=\"inner\"\n",
    ").sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "# Cleaning Fix\n",
    "# Drop invalid or zero/negative prices\n",
    "merged_df = merged_df[(merged_df[\"UK_price\"] > 0) & (merged_df[\"FR_price\"] > 0)]\n",
    "\n",
    "# Drop duplicates just in case\n",
    "merged_df = merged_df.drop_duplicates(subset=[\"datetime\"])\n",
    "\n",
    "# Compute log returns for sanity filtering\n",
    "rets = np.log(merged_df[[\"UK_price\", \"FR_price\"]]).diff()\n",
    "\n",
    "# keep rows where returns are within ±1 (≈±170% per hour), otherwise drop as bad data\n",
    "mask = (rets[\"UK_price\"].abs() < 1) & (rets[\"FR_price\"].abs() < 1)\n",
    "merged_df = merged_df.loc[mask].copy()\n",
    "\n",
    "# Sort again and reset index\n",
    "merged_df = merged_df.sort_values(\"datetime\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             datetime  FR_price  UK_price\n",
      "0 2025-01-01 01:00:00     16.66    70.001\n",
      "1 2025-01-01 02:00:00     13.10    74.074\n",
      "2 2025-01-01 03:00:00      5.90    78.937\n",
      "3 2025-01-01 04:00:00      9.27    62.961\n",
      "4 2025-01-01 05:00:00     10.04    61.395\n",
      "                datetime  FR_price  UK_price\n",
      "4224 2025-07-31 19:00:00    114.45    69.859\n",
      "4225 2025-07-31 20:00:00    110.27    86.063\n",
      "4226 2025-07-31 21:00:00     98.32    96.437\n",
      "4227 2025-07-31 22:00:00    102.22    76.596\n",
      "4228 2025-07-31 23:00:00     93.47    79.206\n",
      "(4229, 3)\n"
     ]
    }
   ],
   "source": [
    "#Inspect results\n",
    "print(merged_df.head())\n",
    "print(merged_df.tail())\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing hours: 858\n",
      "\n",
      "First 20 missing hours:\n",
      "DatetimeIndex(['2025-01-06 03:00:00', '2025-01-06 06:00:00',\n",
      "               '2025-01-07 00:00:00', '2025-01-07 03:00:00',\n",
      "               '2025-01-07 04:00:00', '2025-01-07 05:00:00',\n",
      "               '2025-01-14 03:00:00', '2025-01-20 08:00:00',\n",
      "               '2025-01-24 03:00:00', '2025-01-26 10:00:00',\n",
      "               '2025-01-26 12:00:00', '2025-01-27 02:00:00',\n",
      "               '2025-01-27 03:00:00', '2025-01-27 04:00:00',\n",
      "               '2025-01-27 05:00:00', '2025-01-28 03:00:00',\n",
      "               '2025-01-28 04:00:00', '2025-01-28 05:00:00',\n",
      "               '2025-01-29 02:00:00', '2025-01-29 03:00:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "Missing hours per day (top 20 days with most missing):\n",
      "date\n",
      "2025-06-05    18\n",
      "2025-06-08    15\n",
      "2025-06-03    14\n",
      "2025-05-25    14\n",
      "2025-06-06    13\n",
      "2025-06-12    13\n",
      "2025-03-30    13\n",
      "2025-05-10    12\n",
      "2025-05-06    12\n",
      "2025-06-01    11\n",
      "2025-05-22    11\n",
      "2025-04-06    11\n",
      "2025-07-20    11\n",
      "2025-05-05    11\n",
      "2025-06-09    10\n",
      "2025-05-08    10\n",
      "2025-05-11    10\n",
      "2025-05-18    10\n",
      "2025-06-16    10\n",
      "2025-04-27    10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for missing hours - Test\n",
    "import pandas as pd\n",
    "\n",
    "# Use merged dataset\n",
    "df = merged_df.copy()\n",
    "\n",
    "# Create expected hourly timeline between min and max datetime\n",
    "expected_hours = pd.date_range(df[\"datetime\"].min(), df[\"datetime\"].max(), freq=\"h\")\n",
    "\n",
    "# Identify missing hours\n",
    "missing_hours = expected_hours.difference(df[\"datetime\"])\n",
    "print(f\"Total missing hours: {len(missing_hours)}\")\n",
    "print(\"\\nFirst 20 missing hours:\")\n",
    "print(missing_hours[:20])\n",
    "\n",
    "# Group missing hours by day to see clustering\n",
    "missing_df = pd.DataFrame(missing_hours, columns=[\"datetime\"])\n",
    "missing_df[\"date\"] = missing_df[\"datetime\"].dt.date\n",
    "missing_count_per_day = missing_df.groupby(\"date\").size().sort_values(ascending=False)\n",
    "print(\"\\nMissing hours per day (top 20 days with most missing):\")\n",
    "print(missing_count_per_day.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged hourly dataset saved to /Users/Lyndon.Odia/Desktop/lo-devx/power-spread-option-pricing-main/data/processed/UK_FR_day_ahead_hourly_010125-31072025.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_path = os.path.join(\n",
    "    processed_data_dir,\n",
    "    \"UK_FR_day_ahead_hourly_010125-31072025.csv\"\n",
    ")\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged hourly dataset saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
